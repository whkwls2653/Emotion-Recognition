{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tip-colab-파일저장및업로드-colab",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/whkwls2653/Emotion-Recognition/blob/main/wav2vec_local.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "7-JR7WQ5O70X",
        "outputId": "b5c0ef1c-4c44-427b-96ea-0445a91feb99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "                                                         \n",
        "dataset_train, dataset_test = train_test_split(allfile_datalist, test_size=0.25, random_state=0)\n",
        "print(len(dataset_train))\n",
        "print(len(dataset_test))"
      ],
      "metadata": {
        "id": "WNkKywK09DwZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.11.3\n",
        "!pip install torchaudio\n",
        "!pip install librosa\n",
        "!pip install jiwer\n",
        "!pip install dataset"
      ],
      "metadata": {
        "id": "858CpYNXa-qg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass, field\n",
        "from typing import Any, Dict, List, Optional, Union\n",
        "\n",
        "import torch\n",
        "import ast\n",
        "from transformers import Wav2Vec2Processor\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorCTCWithPadding:\n",
        "    processor: Wav2Vec2Processor.from_pretrained(\"/content/gdrive/MyDrive/감정인식_대회/wave2vec2\", pad_token_id=49)\n",
        "    padding: Union[bool, str] = True\n",
        "    max_length: Optional[int] = None\n",
        "    max_length_labels: Optional[int] = None\n",
        "    pad_to_multiple_of: Optional[int] = None\n",
        "    pad_to_multiple_of_labels: Optional[int] = None\n",
        "\n",
        "    def mapping(self,data):\n",
        "        with self.processor.as_target_processor():\n",
        "            ret = self.processor(\"\".join([i if i!='\\x1b' else '|' for i in ast.literal_eval(data)])).input_ids\n",
        "            ret_torch = torch.tensor([int(0 if value is None else value) for value in ret])\n",
        "        return ret_torch\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        # split inputs and labels since they have to be of different lenghts and need\n",
        "        # different padding methods\n",
        "        input_features = [{\"input_values\": feature['input_values']} for feature in features]\n",
        "        # e.g. feature['label'] = \"ㅇㅏㄴㄴㅕㅇㅎㅏㅅㅔㅇㅛ\"\n",
        "        label_features = [{\"input_ids\": self.mapping(feature[\"label\"])} for feature in features]\n",
        "\n",
        "        batch = self.processor.pad(\n",
        "            input_features,\n",
        "            padding=self.padding,\n",
        "            max_length=self.max_length,\n",
        "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        with self.processor.as_target_processor():\n",
        "            labels_batch = self.processor.pad(\n",
        "                label_features,\n",
        "                padding=self.padding,\n",
        "                max_length=self.max_length_labels,\n",
        "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
        "                return_tensors=\"pt\",\n",
        "            )\n",
        "\n",
        "        # replace padding with -100 to ignore loss correctly\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch\n",
        "\n",
        "\n",
        "# 자소 단위로 나누어지지 않은 경우 사용 -> 한번 하면 저장해놓기\n",
        "class DataProc:\n",
        "    def __init__(self, model_name=\"/content/gdrive/MyDrive/감정인식_대회/wave2vec2\"):\n",
        "        self.processor = Wav2Vec2Processor.from_pretrained(model_name, pad_token_id=49)\n",
        "\n",
        "    def to_jaso(self, sentence):\n",
        "        NO_JONGSUNG = ''\n",
        "        CHOSUNGS = ['ㄱ', 'ㄲ', 'ㄴ', 'ㄷ', 'ㄸ', 'ㄹ', 'ㅁ', 'ㅂ', 'ㅃ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅉ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
        "        JOONGSUNGS = ['ㅏ', 'ㅐ', 'ㅑ', 'ㅒ', 'ㅓ', 'ㅔ', 'ㅕ', 'ㅖ', 'ㅗ', 'ㅘ', 'ㅙ', 'ㅚ', 'ㅛ', 'ㅜ', 'ㅝ', 'ㅞ', 'ㅟ', 'ㅠ', 'ㅡ', 'ㅢ', 'ㅣ']\n",
        "        JONGSUNGS = [NO_JONGSUNG,  'ㄱ', 'ㄲ', 'ㄳ', 'ㄴ', 'ㄵ', 'ㄶ', 'ㄷ', 'ㄹ', 'ㄺ', 'ㄻ', 'ㄼ', 'ㄽ', 'ㄾ', 'ㄿ', 'ㅀ', 'ㅁ', 'ㅂ', 'ㅄ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
        "\n",
        "        N_CHOSUNGS, N_JOONGSUNGS, N_JONGSUNGS = 19, 21, 28\n",
        "        FIRST_HANGUL, LAST_HANGUL = 0xAC00, 0xD7A3 #'가', '힣'    \n",
        "     \n",
        "        result = []\n",
        "        for char in sentence:\n",
        "            if ord(char) < FIRST_HANGUL or ord(char) > LAST_HANGUL: \n",
        "                result.append('|')\n",
        "            else:          \n",
        "                code = ord(char) - FIRST_HANGUL\n",
        "                jongsung_index = code % N_JONGSUNGS\n",
        "                code //= N_JONGSUNGS\n",
        "                joongsung_index = code % N_JOONGSUNGS\n",
        "                code //= N_JOONGSUNGS\n",
        "                chosung_index = code\n",
        "                result.append(CHOSUNGS[chosung_index])\n",
        "                result.append(JOONGSUNGS[joongsung_index])\n",
        "                if jongsung_index!=0:\n",
        "                    result.append(JONGSUNGS[jongsung_index])\n",
        "                \n",
        "        with self.processor.as_target_processor():\n",
        "            ret = self.processor(\"\".join(result))\n",
        "        \n",
        "        return ret.input_ids\n",
        "\n",
        "    def prepare_dataset(self, df):\n",
        "        \"\"\"\n",
        "        df.cols = ['audio', 'sentence', 'path']\n",
        "        \"\"\"\n",
        "        df['label'] = df['sentence'].apply(self.to_jaso)\n",
        "        return df"
      ],
      "metadata": {
        "id": "fGSFs-Z6a-oL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install datasets\n",
        "# import datasets\n",
        "from datasets import load_metric, load_from_disk\n",
        "# from data_proc import  get_senior_data\n",
        "# from data_collator import DataCollatorCTCWithPadding\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import ast\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "class MyDataset(Dataset):\n",
        "  def __init__(self,data_list):\n",
        "    self.data_list=data_list\n",
        "    self.len=len(data_list)\n",
        "  def __getitem__(self, index):\n",
        "    return self.data_list[index]\n",
        "  def __len__(self):\n",
        "    return self.len\n",
        "# import os\n",
        "# os.environ['CUDA_LAUNCH_BLOCKING']='1'\n",
        "# os.environ['CUDA_VISIBLE_DEVICES'] = \"2,3\"\n",
        "\n",
        "repo_name = '/content/gdrive/MyDrive/감정인식_대회/wave2vec2'\n",
        "\n",
        "processor = Wav2Vec2Processor.from_pretrained('/content/gdrive/MyDrive/감정인식_대회/wave2vec2')\n",
        "wer_metric = load_metric(\"wer\")\n",
        "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    pred_logits = pred.predictions\n",
        "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
        "    pred.label_ids[pred.label_ids == -100] = 49 # as fleek model has 2 pad tokens\n",
        "    pred_str = processor.batch_decode(pred_ids)\n",
        "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
        "\n",
        "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "    return {\"wer\": wer}\n",
        "\n",
        "def get_model(model_name=\"/content/gdrive/MyDrive/감정인식_대회/wave2vec2\"): \n",
        "    model = Wav2Vec2ForCTC.from_pretrained(\n",
        "        model_name, \n",
        "        attention_dropout=0.1,\n",
        "        hidden_dropout=0.1,\n",
        "        feat_proj_dropout=0.0,\n",
        "        mask_time_prob=0.05,\n",
        "        layerdrop=0.1,\n",
        "        ctc_loss_reduction=\"mean\", \n",
        "        pad_token_id=49,\n",
        "        vocab_size=50,\n",
        "        ignore_mismatched_sizes=True\n",
        "    )\n",
        "\n",
        "    model.freeze_feature_extractor()\n",
        "    model.gradient_checkpointing_enable()\n",
        "    return model\n",
        "\n",
        "def train_model(train, test, model):\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=repo_name,\n",
        "        group_by_length=True,\n",
        "        per_device_train_batch_size=32,\n",
        "        per_device_eval_batch_size=32,\n",
        "        gradient_accumulation_steps=4,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        num_train_epochs=30,\n",
        "        fp16=True,\n",
        "        save_steps=300,\n",
        "        eval_steps=300,\n",
        "        logging_steps=50,\n",
        "        learning_rate=3e-4,\n",
        "        warmup_steps=300,\n",
        "        save_total_limit=1,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model='eval_loss',\n",
        "        dataloader_num_workers=6\n",
        "        )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        compute_metrics=compute_metrics,\n",
        "        train_dataset=train,\n",
        "        eval_dataset=test,\n",
        "        tokenizer=processor.feature_extractor,\n",
        "        data_collator=data_collator\n",
        "    )\n",
        "    print(f\"build trainer on device {training_args.device} with {training_args.n_gpu} gpus\")\n",
        "    trainer.train()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    \n",
        "    # torch.cuda.empty_cache()\n",
        "    \n",
        "    # dataset = load_from_disk('/content/gdrive/MyDrive/감정인식_대회/데이터셋/2019wav_txt_label_zip_toteval.pkl')\n",
        "    # dataset = dataset.remove_columns(['wav', 'text', 'labels'])\n",
        "\n",
        "    # train = dataset['train'] #.select([i for i in range(0,5000)])\n",
        "    # test =  dataset['valid'] #.select([i for i in range(0,5000)])\n",
        "  wav_txt_label_root='/content/gdrive/MyDrive/감정인식_대회/데이터셋/2019wav_txt_label_zip.pkl'\n",
        "  with open(wav_txt_label_root,'rb') as f:\n",
        "    allfile_datalist=pickle.load(f)\n",
        "\n",
        "  # print(len(allfile_datalist))\n",
        "  # print(allfile_datalist[0])\n",
        "  train=MyDataset(allfile_datalist[:10000])\n",
        "  test=MyDataset(allfile_datalist[10000:])\n",
        "\n",
        "  model = get_model()\n",
        "  train_model(train, test, model)"
      ],
      "metadata": {
        "id": "g0f6Zou4a-mE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1b4b5700-8b26-464d-d8b7-5f1d8a10895d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.9/dist-packages (2.11.0)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (4.65.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.13.3)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (2023.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from datasets) (3.2.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets) (1.4.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from datasets) (23.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (22.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.10.7)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading feature extractor configuration file /content/gdrive/MyDrive/감정인식_대회/wave2vec2/preprocessor_config.json\n",
            "Feature extractor Wav2Vec2FeatureExtractor {\n",
            "  \"do_normalize\": true,\n",
            "  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n",
            "  \"feature_size\": 1,\n",
            "  \"padding_side\": \"right\",\n",
            "  \"padding_value\": 0.0,\n",
            "  \"return_attention_mask\": false,\n",
            "  \"sampling_rate\": 16000\n",
            "}\n",
            "\n",
            "Didn't find file /content/gdrive/MyDrive/감정인식_대회/wave2vec2/tokenizer.json. We won't load it.\n",
            "loading file /content/gdrive/MyDrive/감정인식_대회/wave2vec2/vocab.json\n",
            "loading file /content/gdrive/MyDrive/감정인식_대회/wave2vec2/tokenizer_config.json\n",
            "loading file /content/gdrive/MyDrive/감정인식_대회/wave2vec2/added_tokens.json\n",
            "loading file /content/gdrive/MyDrive/감정인식_대회/wave2vec2/special_tokens_map.json\n",
            "loading file None\n",
            "Adding <s> to the vocabulary\n",
            "Adding </s> to the vocabulary\n",
            "Adding <pad> to the vocabulary\n",
            "loading configuration file /content/gdrive/MyDrive/감정인식_대회/wave2vec2/config.json\n",
            "Model config Wav2Vec2Config {\n",
            "  \"_name_or_path\": \"hyyoka/wav2vec2-xlsr-korean-senior\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"adapter_kernel_size\": 3,\n",
            "  \"adapter_stride\": 2,\n",
            "  \"add_adapter\": false,\n",
            "  \"apply_spec_augment\": true,\n",
            "  \"architectures\": [\n",
            "    \"Wav2Vec2ForCTC\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"classifier_proj_size\": 256,\n",
            "  \"codevector_dim\": 256,\n",
            "  \"contrastive_logits_temperature\": 0.1,\n",
            "  \"conv_bias\": true,\n",
            "  \"conv_dim\": [\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512\n",
            "  ],\n",
            "  \"conv_kernel\": [\n",
            "    10,\n",
            "    3,\n",
            "    3,\n",
            "    3,\n",
            "    3,\n",
            "    2,\n",
            "    2\n",
            "  ],\n",
            "  \"conv_stride\": [\n",
            "    5,\n",
            "    2,\n",
            "    2,\n",
            "    2,\n",
            "    2,\n",
            "    2,\n",
            "    2\n",
            "  ],\n",
            "  \"ctc_loss_reduction\": \"mean\",\n",
            "  \"ctc_zero_infinity\": false,\n",
            "  \"diversity_loss_weight\": 0.1,\n",
            "  \"do_stable_layer_norm\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"feat_extract_activation\": \"gelu\",\n",
            "  \"feat_extract_dropout\": 0.0,\n",
            "  \"feat_extract_norm\": \"layer\",\n",
            "  \"feat_proj_dropout\": 0.0,\n",
            "  \"feat_quantizer_dropout\": 0.0,\n",
            "  \"final_dropout\": 0.0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"layerdrop\": 0.1,\n",
            "  \"mask_channel_length\": 10,\n",
            "  \"mask_channel_min_space\": 1,\n",
            "  \"mask_channel_other\": 0.0,\n",
            "  \"mask_channel_prob\": 0.0,\n",
            "  \"mask_channel_selection\": \"static\",\n",
            "  \"mask_feature_length\": 10,\n",
            "  \"mask_feature_min_masks\": 0,\n",
            "  \"mask_feature_prob\": 0.0,\n",
            "  \"mask_time_length\": 10,\n",
            "  \"mask_time_min_masks\": 2,\n",
            "  \"mask_time_min_space\": 1,\n",
            "  \"mask_time_other\": 0.0,\n",
            "  \"mask_time_prob\": 0.05,\n",
            "  \"mask_time_selection\": \"static\",\n",
            "  \"model_type\": \"wav2vec2\",\n",
            "  \"num_adapter_layers\": 3,\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_codevector_groups\": 2,\n",
            "  \"num_codevectors_per_group\": 320,\n",
            "  \"num_conv_pos_embedding_groups\": 16,\n",
            "  \"num_conv_pos_embeddings\": 128,\n",
            "  \"num_feat_extract_layers\": 7,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_negatives\": 100,\n",
            "  \"output_hidden_size\": 1024,\n",
            "  \"pad_token_id\": 49,\n",
            "  \"proj_codevector_dim\": 256,\n",
            "  \"tdnn_dilation\": [\n",
            "    1,\n",
            "    2,\n",
            "    3,\n",
            "    1,\n",
            "    1\n",
            "  ],\n",
            "  \"tdnn_dim\": [\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    1500\n",
            "  ],\n",
            "  \"tdnn_kernel\": [\n",
            "    5,\n",
            "    3,\n",
            "    3,\n",
            "    1,\n",
            "    1\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.11.3\",\n",
            "  \"use_weighted_layer_sum\": false,\n",
            "  \"vocab_size\": 50,\n",
            "  \"xvector_output_dim\": 512\n",
            "}\n",
            "\n",
            "loading weights file /content/gdrive/MyDrive/감정인식_대회/wave2vec2/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing Wav2Vec2ForCTC.\n",
            "\n",
            "All the weights of Wav2Vec2ForCTC were initialized from the model checkpoint at /content/gdrive/MyDrive/감정인식_대회/wave2vec2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Wav2Vec2ForCTC for predictions without further training.\n",
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "Using amp fp16 backend\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "build trainer on device cuda:0 with 1 gpus\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-88f07eee2e40>\u001b[0m in \u001b[0;36m<cell line: 95>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m   \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-26-88f07eee2e40>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(train, test, model)\u001b[0m\n\u001b[1;32m     91\u001b[0m     )\n\u001b[1;32m     92\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"build trainer on device {training_args.device} with {training_args.n_gpu} gpus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m         \u001b[0;31m# Data loader and number of training steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1115\u001b[0;31m         \u001b[0mtrain_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_train_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1117\u001b[0m         \u001b[0;31m# Setting up training control variables:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mget_train_dataloader\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    649\u001b[0m             )\n\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m         \u001b[0mtrain_sampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_train_sampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m         return DataLoader(\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_get_train_sampler\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0mmodel_input_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_input_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld_size\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 574\u001b[0;31m                 return LengthGroupedSampler(\n\u001b[0m\u001b[1;32m    575\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_batch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/trainer_pt_utils.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, batch_size, lengths, model_input_name, generator)\u001b[0m\n\u001b[1;32m    535\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_input_name\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m             ):\n\u001b[0;32m--> 537\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    538\u001b[0m                     \u001b[0;34m\"Can only automatically infer lengths for datasets whose items are dictionaries with an \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m                     \u001b[0;34mf\"'{self.model_input_name}' key.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Can only automatically infer lengths for datasets whose items are dictionaries with an 'input_values' key."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a=allfile_datalist[0]\n",
        "print(a)\n",
        "for a in allfile_datalist:\n",
        "  a[0],a[1],a[2]=a[1],a[0],a[2]\n",
        "\n",
        "print(allfile_datalist[0])\n",
        "print(allfile_datalist[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osgPm65t6bx7",
        "outputId": "51c5360e-7a36-4b0d-ca2f-bbd6fb4a99cb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['어 저 지그 지금 사람 친 거야? 지금 사람 친 거 맞지? 그치?\\n', '/content/gdrive/MyDrive/감정인식 대회/데이터셋/KEMDy19/wav/Session01/Sess01_script01/Sess01_script01_M001.wav', '1']\n",
            "['/content/gdrive/MyDrive/감정인식 대회/데이터셋/KEMDy19/wav/Session01/Sess01_script01/Sess01_script01_M001.wav', '어 저 지그 지금 사람 친 거야? 지금 사람 친 거 맞지? 그치?\\n', '1']\n",
            "['/content/gdrive/MyDrive/감정인식 대회/데이터셋/KEMDy19/wav/Session01/Sess01_script01/Sess01_script01_F001.wav', 'b/ 몰라. o/ b/ 아 몰라 어떡해. o/\\n', '0']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "with open('/content/gdrive/MyDrive/감정인식_대회/데이터셋/2019wav_txt_label_zip.pkl','wb') as f:\n",
        "  pickle.dump(allfile_datalist,f)"
      ],
      "metadata": {
        "id": "SLSTr_rsqjxN"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h3BLdqy7a-j-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7BBac7CPa-gq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LuspU6xca-Ym"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}